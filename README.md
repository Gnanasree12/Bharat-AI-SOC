# Bharat-AI-SOC
Hardware-Accelerated CNN Inference on Zynq SoC
ğŸ“Œ Project Title

Hardware-Accelerated CNN Inference System on Xilinx Zynq SoC using FPGA Fabric

ğŸ¯ Objective

Design and implement a hardware-accelerated convolutional neural network (CNN) inference system on a Xilinx Zynq SoC, leveraging FPGA fabric to achieve real-time image classification or object detection and demonstrate performance improvement over CPU-only execution.

ğŸ“– Project Description

This project focuses on accelerating edge AI workloads on embedded platforms using hardware/software co-design. A lightweight CNN model is deployed on a Zynq-based system where the ARM processor handles control and preprocessing, while FPGA fabric accelerates compute-intensive CNN operations such as convolution, activation, and pooling.

The goal is to enable real-time inference on embedded hardware and compare performance with a CPU-only implementation.

ğŸ§  System Architecture

The system partitions functionality between ARM and FPGA:

ARM Processor

Image capture / dataset input

Preprocessing (resize, normalize)

Control logic

Post-processing & output

FPGA Fabric

Convolution accelerator

Activation (ReLU)

Pooling

Feature map processing

Input images are processed by the ARM core and offloaded to FPGA for accelerated CNN inference, then results are returned for classification/detection output.

ğŸ§° Hardware Platform

Target platform: Digilent Zybo (Zynq-7000 SoC)

ğŸ§ª CNN Model

Lightweight CNN model suitable for embedded inference (Tiny-YOLO / MobileNet / custom CNN).

Layer structure:

Convolution

ReLU activation

Pooling

Fully connected

Classification output

âš™ï¸ FPGA Implementation

CNN compute-intensive operations are implemented as hardware accelerators using:

Vivado / Vitis HLS

Verilog / HLS C++

Parallel convolution architecture

Pipelining for throughput optimization

ğŸ’» Software Implementation

Embedded software on ARM processor performs:

Image preprocessing (OpenCV)

Data transfer to FPGA

Accelerator control

Result retrieval

Classification / detection output

ğŸš€ Performance Goals

Real-time or near real-time inference

â‰¥ 2Ã— speedup over CPU-only CNN

Reduced latency

Improved throughput

Better power efficiency

ğŸ“Š Results

The CNN accelerator was successfully designed and synthesized for the Zynq FPGA platform. Bitstream generation and hardware/software co-design flow were completed.

âš ï¸ Note

Due to USB-JTAG detection issues on the host system, live FPGA programming could not be demonstrated. However, the complete hardware-accelerated CNN architecture and FPGA accelerator design were successfully implemented in the development environment.

ğŸ“‚ Repository Structure
fpga_design/  
cnn_model/  
software/  
constraints/  
bitstream/  
docs/  
results/  

ğŸ‘©â€ğŸ’» Team Members

G. Ruthik

K. Varun

C. Gnanasree

Electronics & Communication Engineering (ECE)

ğŸ“ Learning Outcomes

Embedded edge-AI on Zynq SoC

FPGA-based CNN acceleration

ARMâ€“FPGA hardware/software co-design

Performance optimization of embedded AI systems

ğŸ“œ License

Academic and educational use only.
